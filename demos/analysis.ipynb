{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d80db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from wcbtfidf import Wcbtfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21504544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sentiment140_data.csv',names=('target','id','date','flag','username','tweet'))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222cdb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "          username                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24aaa807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1598315 1600000\n"
     ]
    }
   ],
   "source": [
    "# Checking unique ids\n",
    "print(df['id'].nunique(),df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ab6dfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1598315 1598315\n"
     ]
    }
   ],
   "source": [
    "# Removing duplicate ids\n",
    "df.drop_duplicates(subset=['id'],keep='first',inplace=True)\n",
    "print(df['id'].nunique(),df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd9677f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.500527\n",
       "4    0.499473\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target data distribution\n",
    "df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb7d513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.9\n",
       "0    0.1\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To test our hypothesis let us convert into an imbalance problem with fewer positive samples\n",
    "# We will take a total of 5 lakh data points with 4.5 lakh belonging to class 4 and 50k  to class 0\n",
    "\n",
    "negative_samples = df[df['target'] == 0].sample(n=50000,random_state=60)\n",
    "positive_samples = df[df['target'] == 4].sample(n=450000,random_state=60)\n",
    "\n",
    "final_df = pd.concat([negative_samples,positive_samples]).sample(frac=1,random_state=60) # A sample operation with full data is \n",
    "                                                                         # performed to shuffle the data points\n",
    "\n",
    "final_df['target'] = final_df['target'].map({0:0,4:1})\n",
    "final_df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c06a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-z0-9]\",\" \",text)\n",
    "    text = re.sub(\"(\\s)+\",\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "889956c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['clean_text'] = final_df['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37761aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 7)\n",
      "(500000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(final_df.shape)\n",
    "final_df = final_df[['clean_text','target']]\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f07f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375000,) (375000,)\n",
      "(125000,) (125000,)\n"
     ]
    }
   ],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(final_df['clean_text'],final_df['target'],test_size=0.25,random_state=60,stratify=final_df['target'])\n",
    "\n",
    "print(xtrain.shape,ytrain.shape)\n",
    "print(xtest.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a6f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.9\n",
      "0    0.1\n",
      "Name: target, dtype: float64\n",
      "1    0.9\n",
      "0    0.1\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Distribution check in train and test\n",
    "\n",
    "print(ytrain.value_counts(normalize=True))\n",
    "print(ytest.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1559cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hypothesis(xtrain,xtest,ytrain,ytest,max_feat,model):\n",
    "    \n",
    "    print('Running base version')\n",
    "    tfidf = TfidfVectorizer(max_features=max_feat,stop_words='english')\n",
    "    train_df = pd.DataFrame(tfidf.fit_transform(xtrain).toarray(),columns=tfidf.vocabulary_)\n",
    "    test_df = pd.DataFrame(tfidf.transform(xtest).toarray(),columns=tfidf.vocabulary_)\n",
    "    \n",
    "    \n",
    "    model.fit(train_df,ytrain)\n",
    "    preds = model.predict(test_df)\n",
    "    print(f'Precision is {precision_score(ytest,preds)}')\n",
    "    print(f'Recall is {recall_score(ytest,preds)}')\n",
    "    print(f'ROC curve is {roc_auc_score(ytest,preds)}')\n",
    "    print(classification_report(ytest,preds))\n",
    "    \n",
    "    print('Running my version')\n",
    "    wcbtfidf = Wcbtfidf(max_features=max_feat)\n",
    "    wcbtfidf.fit(xtrain,ytrain)\n",
    "    \n",
    "    train_df = wcbtfidf.transform(xtrain)\n",
    "    test_df = wcbtfidf.transform(xtest)\n",
    "    \n",
    "    model.fit(train_df,ytrain)\n",
    "    preds = model.predict(test_df)\n",
    "    print(f'Precision is {precision_score(ytest,preds)}')\n",
    "    print(f'Recall is {recall_score(ytest,preds)}')\n",
    "    print(f'ROC curve is {roc_auc_score(ytest,preds)}')\n",
    "    print(classification_report(ytest,preds))\n",
    "    return wcbtfidf,tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d44d6539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running base version\n",
      "Precision is 0.9077311798254879\n",
      "Recall is 0.9931466666666666\n",
      "ROC curve is 0.5422933333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.09      0.16     12500\n",
      "           1       0.91      0.99      0.95    112500\n",
      "\n",
      "    accuracy                           0.90    125000\n",
      "   macro avg       0.75      0.54      0.55    125000\n",
      "weighted avg       0.88      0.90      0.87    125000\n",
      "\n",
      "Running my version\n",
      "Precision is 0.9100734976221357\n",
      "Recall is 0.9916888888888888\n",
      "ROC curve is 0.5548844444444444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.12      0.20     12500\n",
      "           1       0.91      0.99      0.95    112500\n",
      "\n",
      "    accuracy                           0.90    125000\n",
      "   macro avg       0.76      0.55      0.57    125000\n",
      "weighted avg       0.88      0.90      0.87    125000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "wcbtfidf_object,tfidf_object = check_hypothesis(xtrain,xtest,ytrain,ytest,300,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7981fe",
   "metadata": {},
   "source": [
    "## ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b620ecd",
   "metadata": {},
   "source": [
    "Negative tweets are the minority class. Let us see whether the vocab of wcbtfidf was able to catch words that cater towards the negative class more as compared to tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0b5fa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300\n"
     ]
    }
   ],
   "source": [
    "# Length Comparison\n",
    "\n",
    "tfidf_vocab = tfidf_object.vocabulary_\n",
    "wcbtfidf_vocab = wcbtfidf_object.combine_vocab\n",
    "\n",
    "print(len(wcbtfidf_vocab),len(tfidf_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc686a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['remember', 'hopefully', 'stop', 'tv', 'did', 'kids', 'rock', 'having', '100', 'send', 'came', 'taking', 'tho', 'beach', 'enjoying', 'true', 'seen', 'says', 'just', 'stay', 'lmao', 'kind', 'using', 'mileycyrus', 'saturday', 'idea', 'dad', 'loved', 'outside', 'doing', 'quite', 'plurk', 'crazy', 'ill', 'don', 'room', 'meet', 'watched', 'totally', 'talking', 'ah', 'guy', 'shopping', 'wonderful', 'breakfast', 'end', 'years', 'probably', 'lots', 'till', 'believe', 'fine', 'super', 'fm', 'cause', 'pics', 'hour', 'busy', 'rest', 'mind', 'weeks', 'buy', 'does', 'girls']\n"
     ]
    }
   ],
   "source": [
    "# Words that are present in tfidf vocab but not in wcbtfidf\n",
    "\n",
    "print(list(set(tfidf_vocab) - set(wcbtfidf_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b765bd",
   "metadata": {},
   "source": [
    "Major words are neutral and rest are positive like **rock,enjoying,loved,wonderful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56346eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['give', 'stupid', 'may', 'didnt', 'nothing', 'yet', 'always', 'well', 'find', 'sucks', 'ever', 'never', 'keep', 'around', 'another', 'almost', 'ugh', 'cold', 'done', 'many', 'hurts', 'one', 'two', 'back', 'show', 're', 'cant', 'everything', 'something', 'take', 'could', 'would', 'shit', 'put', 'everyone', 'already', 'see', 'even', 'though', 'get', 'might', 'headache', 'still', 'gone', 'first', 'must', 'last', 'go', 'someone', 'since', 'much', 'wanted', 'iphone', 'poor', 'call', 'next', 'also', 'please', 'made', 'least', 'name', 'missing', 'found', 'us']\n"
     ]
    }
   ],
   "source": [
    "# Words that are present in wcbtfidf but not in tfidf\n",
    "\n",
    "print(list(set(wcbtfidf_vocab) - set(tfidf_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe5555",
   "metadata": {},
   "source": [
    "Here as well there are neutral words but rest are towards the negative end like **stupid,sucks,ugh,hurts,shit,headache,poor,least,missing**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
